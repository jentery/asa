# Notes toward Speculative Computer Vision (Draft) 
2016 Meeting of the American Studies Association | Jentery Sayers 

Computer vision is generally associated with the programmatic description and reconstruction of the physical world in digital form (Szeliski 2010: 3-10). It helps people construct and express visual patterns in data, such as patterns in image, video, and text repositories. The processes involved in this recognition are incredibly tedious, hence tendencies to automate them with algorithms. Such processes are also increasingly common in everyday life, expanding the role of algorithms in the reproduction of culture. From the perspective of economic sociology, A. Aneesh links such expansion to "a new kind of power" and governance, which he refers to as "*algocracy*—rule of the algorithm, or rule of the code" (Aneesh 2006: 5). Here, the programmatic treatment of the physical world in digital form is so significantly embedded in infrastructures that algorithms tacitly shape behaviors and prosaically assert authority through bureaucracy. Routine decisions are delegated (knowingly or not) to computational procedures that, echoing the work of Alexander Galloway (2001) and Wendy Chun (2011), run in the background as protocols or default settings.

As an example, consider the banal and now ubiquitous integration of CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) into online submission forms. By prompting you to type obscure letters into a box, CAPTCHAs ask, "Are you human?" In so doing, they regulate information flow and implicitly assume quite a bit about people (e.g., that "humans" are not blind, do not have low vision, or do not have dyslexia). In many cases, such as Google’s reCAPTCHA initiative, CAPTCHAs also assist computing projects in their digitization, annotation, and machine-learning efforts. When computer vision cannot identify a visual pattern, CAPTCHAs forward the pattern to people for manual remediation into distinct alphanumeric characters. This data is then integrated back into collections of electronic texts or used to train algorithms to interpret more content. Understood this way, a person's response to "are you human?" is also a gesture that helps the machine. In economic terms, these quotidian exchanges are mutually beneficial, or at least efficient: CAPTCHA algorithms protect websites from malicious bots and optical character recognition (OCR) attacks, while people provide desired information and verification without consuming much of their own time and effort. Once habituated online, such algocratic tasks become inconsequential in the instance yet extremely meaningful, and strikingly productive of value, in the aggregate.

But my argument today does not focus on algocracy or even CAPTCHAs, even if it is deeply informed by them both. In short, it is about *the need for computer vision techniques that privilege speculation over confirmation*. It is not invested in delegating authority to algorithms; it is also not about mobilizing that vision toward efficiency, or even about foregrounding patterns in repositories, either. It does not, for intance, encourage the algorithmic discovery of historical causes that, until now, have rested ghost-like beyond the threshold of human perception. Instead, my argument is that *speculative approaches to computer vision may resist dichotomizing media and mediation, digital and analog materials, automated and manual labor, and human and machine phenomenologies*. Rather than capturing or re-presenting the world, speculative computer vision may also help people develop a "critical technical practice" (Agre 1997: 155) advocated by scholars such as Tara McPherson (2012) and Alan Liu (2012). For example, it may begin by relinquishing any assumptions that computer vision can be fully understood as, or simply reduced to, "source code." Computer vision necessarily involves lived experience and embodied politics, even when both are dismissed in the name of immediacy and speedy processing. In fact, a brief history of computer vision across the disciplines demonstrates how important lived experience and embodiment are to such processing, and thus to speculative computer vision. 

# Computer Vision Across the Disciplines 

According to various accounts, computer vision research began as early as 1966, during the "Summer Vision Project," when Marvin Minsky, Seymour Papert, Gerald Jay Sussman, and others in the Artificial Intelligence Group (AIG) at the Massachusetts Institute of Technology (MIT) investigated how to use figure-ground analysis to automatically divide pictures into regions based on surface and shape properties. This region description would act as the basis for object identification, where items in pictures were recognized and named by machines with controlled vocabularies of known objects (Papert 1966; Crevier 1993; Boden 2006). Cameras were attached to computers in order to achieve this automated description and identification, with an express desire to eventually construct a pattern analysis system that would combine "maximal flexibility" with "heuristic rules" (Papert 1966: 6).

Although computer vision has developed significantly since the 1960s and ’70s, the AIG’s Summer Vision Project marks a notable transition in media history, a moment when researchers began integrating image processing into the development of artificial intelligence, including the training of computers to read, detect, and describe aspects of pictures and visual environments (Szeliski 2010: 7-10). During the project, AIG researchers also started asking computer vision questions that, if only tacitly, have persisted: How does computer vision differ from human vision? To what degree should computer vision be modeled on human phenomenology, and to what effects? Can computer or human vision even be modeled? That is, can either even be generalized? Where and when do issues of processing and memory matter most for recognition and description? And how should computer vision handle ambiguity? (Minsky 1974). These questions are at once technical and ideological, as are many responses to them, meaning computer vision (then or now) should not be extracted from the contexts of its conceptual and material development.

Today, computer vision has moved, at least in part, from laboratories into consumer technologies. One popular application is (again) answering the question, "Is this you?" or "Is this them?" iPhoto, Facebook, and Kinect users are intimately familiar with this application, where face detection algorithms analyze patterns to calculate a core or integral image within an image, assess differences across a spectrum of light, and view images across scales. In the open source community, many practitioners combine the Open Source Computer Vision (OpenCV) library with the Python, C++, and Java programming languages to perform this "detection" work. These scripts rely on frameworks to train classifiers to detect "objects," which, in the language of vision science, include faces, bodies, and body parts in images based on cascades of features. To see faces while algorithmically scanning images, OpenCV uses the widely popular Viola-Jones object detection framework that relies on "Haar-like" image features for cascading (Viola and Jones 2004). Similar to iPhoto and other image management applications, OpenCV can be used to "identify," often with errors and omissions, the same face across a distribution, even when multiple faces appear in the same image. Here, I surround terms such as detection and identify with quotation marks because, while this is the langauge used in vision science, computer vision actively helps produce the patterns it ostensibly discovers in people, activities, and environments. In order to recognize patterns, it must first perform a translation or remediation into data. The patterns are not somehow inherent to who or what it sees. 

Even more important, *to write computer vision scripts, programmers do not even need to know or even understand the particulars of Haar cascades or Viola-Jones*. Their scripts simply call and deploy "trusted" cascades (e.g., "Frontal Face," "Human Body," "Wall Clock," and "Mouth") stored in XML files readily available across the web. Once a computer vision script detects objects in a given cascade, it can extract them from their source files and archive them. 

Computer vision techniques may also merge or compare extracted objects with existing objects. Comparisons allow people to confirm or complicate known relationships between objects. For instance, when comparing faces, multiple photos of the same person can train algorithms to recognize an "eigenface" (or "unique face") generated from the principle components of those photos. Although eigenfaces do not actually exist in any lived, social reality, they are fundamental to the process of face recognition, and datasets with "training face" images for 100+ people per repository are now common online. One of the most popular sets is the Public Figures Face Database (Pubfig), a "real-world face dataset" consisting of 58,797 internet images of 200 "non-cooperative subjects" that were "taken in completely uncontrolled situations" (Columbia University 2010). While this and other face datasets suggest that training faces are central to big data initiatives anchored in computer vision, *few practitioners have considered the social and cultural implications of treating bodies as big data for vision science*. Indeed, much more research is needed in this area, especially as it relates to policing and racial profiling.

It is also important to note that computer vision responses to "Is this you?" or "Is this them?" do not stop at recognition or pattern analysis after the fact. They enable predictive modeling and forecasting. For example, in surveillance and forensics industries, snapshots are extracted from video and stitched together to articulate "people trajectories," which both archive and anticipate people's movements over time (Calderara et al. 2009: 13-18). Here, the image processing tradition of photogrammetry is clearly linked with artificial intelligence research. As computer vision stitches together a series of objects, it also learns from them and makes suggestions based on them, possibly in real-time. The programmatic description and reconstruction of the physical world are thus directed at the past as well as the future, only heightening their influence in an algocratic paradigm.

Yet, as is evident in the case of unmanned aerial vehicles (UAVs) or drones, computer vision has real-time intelligence applications, too. Many people are familiar with government and private sector investments in UAVs (including military, surveillance, war, and profiling applications), even if they are not familiar with how UAVs work or how computer vision is constructed. But, as with OpenCV research, many people are building their own drones using low-cost microprocessors, sensors, and actuators. They can be programmed to follow scripted missions along a series of waypoints, much in the same way one follows directions from a mobile phone when driving a car, allowing the UAV to detect, track, and respond to objects using OpenCV or another library. In so doing, developers directly link recognition to reaction, tightening the loop between pattern analysis and responsive behaviors to demonstrate machine intelligence.  

Bundled together, the emergence of these techniques raises not many questions about *how computer vision techniques normalize and regulate activities and environments*, including questions about the relevance of computer vision to privacy and social justice issues. At the same time, many developers are researching computer vision applications in a liminal space between standardized and experimental practice, where the consequences remain uncertain or undefined by policy. 

To better understand that liminal space, consider the traction computer vision is gaining in the arts, particularly the combination of machine phenomenology with experimental network aesthetics. Matt Jones (2011) suggests that computer vision corresponds with a "sensor-vernacular aesthetic," or with "optimised, algorithmic sensor sweeps and compression artefacts" (Jones 2011). Somewhere between bits and atoms, a sensor-vernacular aesthetic is "an aesthetic born of the grain of seeing and computation," with David Berry (2012) pointing to a renaissance of 8-bit visuals, the emergence of "pixel fashion," and, generally speaking, a widespread obsession with seeing like a machine. Think Minecraft and decimated meshes on Thingiverse, or Timo Arnal's robot-readable world (2012), Martin Backes's pixelhead (2010), Adam Harvey's stealth wear (2013), and the machine wanderings of James Bridle's "New Aesthetic" (2011).

Whatever the label or example, such aesthetics have largely revised the notion of technologies as "extensions of man" to suggest that computer vision now supplants human vision. In this sense, they are at once humanist, non-humanist, and object-oriented aesthetics. They throw the very notion of human perspective into relief, understanding computer vision as withdrawn, as beyond human access, as some sort of algorithmic unconscious. At the same time, they demand consensus about what human perception entails in the first place. *They need "human" to operate as a stable (or ahistorical, or normative, or universal) category in order to displace it with a computer’s phenomenology.* In the last instance, they are largely reactive in character. Their machine wanderings and robot-readable worlds tend to wonder at machine vision: to suspend "Is this you?" from its social dynamics, without systematically intervening in its cultural functions.

Rather than merely hacking computer vision, repurposing scripts, or fetishizing machine perspectives, maybe *the most pressing challenge for critical approaches to computer vision and algocracy is shifting from a tactical reaction to a strategic articulation of vision infrastructures*. To be sure, this is no small task. From my perspective, it would involve interrogating cascading classifiers for their biases, much in the way Simone Browne (2010) has approached video surveillance, race, and biometrics. After all, numerous examples from software development (e.g., by Flickr, Microsoft, and Hewlett Packard) already point to the racism at work in computer vision algorithms and the interpretation of their results. Critiques of vision infrastructures might also involve reframing computer vision to such a degree that it refuses to establish essentializing, binary ways of seeing (Berger 1972). In other words, amidst the possibilities of using computer vision for oppressive purposes (e.g., its applications for surveillance and racial profiling), we need vision infrastructures that value ambiguous or uncertain vision (a realism of phenomena, if you will), much like Donna Haraway's early work (1985/2003) on cyborgs, feminism, and informatics, including (lest that often overlooked section of "A Cyborg Manifesto" be forgotten) her remarks about an informatics of domination. Her concerns there deeply resemble recent concerns about algocracy (or again, what Annesh defines as "rule of the algorithm, or rule of the code"). 

In humanities research, we see some steps toward these vision infrastructures. Although the discipline has privileged the practical use of OCR to digitize, encode, search, and discover texts, it has also pushed machine vision toward some more speculative applications, which allow scholars to interpret or, better yet, argue with computers. Here, an example is the Real Faces of White Australia project (2011) by Kate Bagnall and Tim Sherratt.  

Real Faces uses a face detection script to foreground the indigenous Australians and non-Europeans not only ignored by the whitewashing of Australia's recordkeeping, but historically subjected to discrimination via the White Australia Policy. In doing so, the project blends an intervention in Australia's archives with a redefinition of vision science. Scripts typically deployed for surveillance and military purposes are instead imagined as mechanisms for critical race studies. At the same time, Bagnall and Sherratt's use of Python and OpenCV for this intervention prompts important questions about *how race is interpreted as form through biometrics and eigenfaces*. For instance, whose faces are idealized or overlooked by machine vision? *How exactly is race a principle component of the training process?* When applied to not only archives but also the everyday spaces of lived experience (e.g., airports, social networks, city streets, and videogames), what forms of racism does computer vision create, and what forms of oppression (e.g., white supremacy) does it enable? By stressing the socio-cultural dimensions of face description and reconstruction, these questions allow us to avoid neatly parsing human and computer vision, or politics and aesthetics, and to refrain from delegating interpretative authority and scholarly responsibility to algorithms. They also refuse attempts to divide historical research with technologies from the simultaneous use of those technologies for routine surveillence and oppression. 

# Next Steps
